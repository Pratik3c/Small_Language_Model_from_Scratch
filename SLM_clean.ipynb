{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijDG3IDqkjKx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Let us build a Small Language Model (SLM) from scratch. We will try to keep the parameter size to 50-60 million.\n",
        "\n",
        "Our goal is to generate creative and coherent text based on the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import the Dataset\n",
        "\n",
        "TinyStories is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We can get it from HuggingFace."
      ],
      "metadata": {
        "id": "tPl23BNsRqfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSeXbOztRtKN",
        "outputId": "ca0649b7-c955-4921-ecdf-85f1754e9d4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkpPWqR8-tFO",
        "outputId": "2bee6e13-8a72-460b-fa00-91a3069ba286"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "6eacfb437f7e4a3499ecb67ecf599f24",
            "d885f2524ffe458db6ea41bb6813cf53",
            "ca41098505194c1bbb0afebdea1f9b13",
            "1a0d3eb2494a411a97ee63245d73656d",
            "f1becb59285f4774932a19971981e7ad",
            "e75be61a0d304a69a8b45705df7e664c",
            "29edab72469f48a9a8c944eba19ce8f0",
            "f52a3a6c92f24ddca124c80f38b6fb8f",
            "092c4fd787154d188fa67081ddb5ffa7",
            "717ca3028c3e4830944659b83dc58311",
            "eb9c8ba2455c4e04b2791061b80a1ed3",
            "d6c7ebc95f8c4d03846f550d6ac111c3",
            "e82bc150b6f1433bb7ac8a12ed55a837",
            "8f2f2484880242a6a8b81736c5945cc2",
            "9fea22d469de452ba49ce9a507eb4f30",
            "e95fced66659466f9c9ff7e9121c69f3",
            "59eba4e637664375980208e33e4eeaa4",
            "b41f96015c3f4a44a41ac4aac986f9c4",
            "1251d399779d4e21a4761231b03a1d7c",
            "411b8f8c8ede42f693229d0b9e015303",
            "1da1ad342c7948afbd8f371d5abbc519",
            "d7e732f6599f417c93813d16b84a7ef7",
            "01298ced92c645479db8b6ed244da510",
            "7050e607458640f7824b082fdf96a525",
            "ab2aee69e0e24be4bdfb817658643d67",
            "888201f01c0e4b2992399107bfd30f3b",
            "f883661c413349c28e3aeca22deb0d98",
            "a020fafa6f0a48f6b2a2c30bade9eba1",
            "1bec05e66eba4f3c897652fadfc8d1de",
            "b7b4d51a5516474d94f7ca6956e29b62",
            "2904848fef924e02b53a0e7942e9fe4d",
            "442912b63ad146398794fd9d123ac9c4",
            "4634dab4e6ef47cf8459a68f43342739",
            "3bfcb3e582fc4cdc90f96c9e7fe4ff64",
            "6fee1aaa6c7147ce96b477d23ac1551d",
            "c229489e89ed427593117f7c8e669273",
            "046c5b45ee804d8687e6b03391a6c557",
            "b3d7f2288c2140cfa871f003209ee324",
            "6f42ebebb43849909a8ca1400a3b6f31",
            "9f7938e243414bf0ae680f7fb8e799b0",
            "e47d4371d5374534a68d9c6b5b6016c7",
            "b3841d38d7b948d0a8a554ee14c36e14",
            "8d3d75ea1dfe493e9f6caf12514c5fa3",
            "97304a2fd3564c89906f5d410a1210da",
            "628ba0c58da24cacb9804a74f94e5dc7",
            "a8ba2e1b697f4479854a45b4465cbaa6",
            "5e0c56cf06e449d899393c4e5cae0a6b",
            "10b734290d804a6c99628417013ae478",
            "0d0b5ba47e8f4eb0a9ec87d34300f35d",
            "c9155b9791114a468def2e18952e05a3",
            "a8a3fdcde66c4fa98853c7f0192a15fb",
            "46907b023657415ab69e1ebc48175b68",
            "9af48b827d94417da74948e59e8bec32",
            "d5feac6021b5447eb58760de828f8e3c",
            "0aebcbbc182f4c52a731505cf3011f3f",
            "503d356905f748f0b5d4eb23ed4f9eb7",
            "8465c442afd3465595f6c9e5ef357032",
            "61eaee74b33443fd95555fd868b6ffd0",
            "e5c47aaf64174444ba8a26fd7405da75",
            "6381f70d8d4a45db92a930796d760c69",
            "e7c1339e8f0640039f67bef7a374bdf2",
            "8d262b81b7d0459f912160ffbf7f12e3",
            "2dc2e6680eed4c0a8d25b3529dbc8585",
            "38b2b7cc98f1493b98bbdec3519676ba",
            "de32230aa69b4ce9889a527e1c9acf01",
            "89009fd78edd4525bb674ed5e120c5fe",
            "fa33719d1a354254a32af783d88f7dcb",
            "4f41b9aa2f3240758f9f8b745f036deb",
            "50fbef39baa84d70ae5c82bc412fd16b",
            "f260281759d44faf89b6fd771e9cd509",
            "46bac1a95d2e4c4eb80d0c3a2c8a9831",
            "da4c8961e5d2492c88a20ce64596075e",
            "33d21eead4d94ced9f14977146960f65",
            "92206dc8b5844a209ccf882f48dffe23",
            "430349e59f2d4ca9a66209d504093375",
            "2e743ace361045c8bc85df05b43b9b03",
            "2d1fda23cad648318f38e78f5a4e81c9",
            "de3a85daf7ea48569375de7c5d23d398",
            "cd08499d2c1a44f89fb80c040d79ffa5",
            "c4cc051f54e241c996d94cb2e7315e12",
            "e2afc748b7614dd490f232a1fa70efc1",
            "f4e12a11c3b44328ae41ef139f02d5a4",
            "87a4b8fa3fe74132a1ac3cc1db2a6e67",
            "774bb57bd96e4a599773fc3b206b19c1",
            "33fb32ad195c479daff571b70ed62ad6",
            "5b3d1fe2021a4b7b9346ab9c4a709cd7",
            "5695e3cc78ea411983b5c3b599fba151",
            "d386de9f2ef0473d9d034d94323e9e22"
          ]
        },
        "id": "3r34TTsnR3GI",
        "outputId": "8cec7ffb-b1fb-4ce7-db49-a758d087d466"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Tokenize the Dataset\n",
        "\n",
        "In this step, we will do the following:\n",
        "\n",
        "(1) Tokenize the dataset into tokenIDs.\n",
        "\n",
        "(2) Create a file called \"train.bin\" and \"validtion.bin\" where we will store the tokenIDs from the entire dataset.\n",
        "\n",
        "(3) We make sure the tokenIDs are stored on a disk, rather than on the RAM for efficient computations."
      ],
      "metadata": {
        "id": "vccyr4qKR6OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(\n",
        "        process,\n",
        "        remove_columns=['text'],\n",
        "        desc=\"tokenizing the splits\",\n",
        "        num_proc=8,\n",
        "        )\n",
        "    # concatenate all the ids in each dataset into one large file we can use for training\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "\n",
        "        idx = 0\n",
        "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "            # Batch together samples for faster write\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "            # Write into mmap\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "568af1a6c8414aa887a3ae75759db570",
            "728928656b7c49a2b3c30c68a8e2c01f",
            "36ff07fa8d844f8a841a019e76355d93",
            "1625a12933094f90a8656441bff1d793",
            "829c95ec9aad45b983752d2c62b4a063",
            "bb3ee3c0b797492391d743733e07b1c6",
            "2c62046603bb40ceb6c7e569dab34e3c",
            "49868797cd7841c986fabf057ffd65b8",
            "7e9626da79df4ec6a372b48b31e05b16",
            "f8710041f0b0443cbe37362330a73a81",
            "3d2e5f74c0974b3c937e74bde5389851",
            "646726401e704c2e81d84e492acdd118",
            "bd7dc20faaa24b40a12e70a12471c395",
            "84acfa6600704d09817b9e4c76765a93",
            "754eb065d62a4c2fa2c1843fd0c74506",
            "78db51aa12384ee5bf70067b47287a6c",
            "11a4d8bb878645809aa57be98b08d800",
            "9c69024bb8c94aeda08dd5bed1a5514d",
            "63106bc643804aa28fd7599148e6203f",
            "59b520b0229343b084f37af381d21037",
            "63aadacc58114ea681d018d07ae9ecb9",
            "218a8bc11b0f4a9eb0a9788518e7356f",
            "0175919826a740958a73de795145bd1b",
            "7a0be8053fe2435cb3b2656ad3143359",
            "6877f5b1f0ee498fab3967de33d27c05",
            "14961ae605ba44acb28960073a6721c7",
            "cd33fcdd87274d4c808b5338114bc14c",
            "b20a59c167584568a9bbac2c9fbcd0bb",
            "a2b810ddcd5449bdbc5986be64e16497",
            "0e6f65244141474298b0c7a046e7ef26",
            "6ee29f2e3f2448ac87aeb1d5de239022",
            "9e9b74f213c647fcaf54e4a5ca660702",
            "532c6fd824634257a38c4e3a9aa1391e",
            "391c5b0d01104290972c2dc6a579dc59",
            "eaca6b3bdf9b440daef57606884a6a80",
            "7482af8605e5470eb06911a5e2f24c60",
            "b6c1f6044a6242cebde6808f27402085",
            "1279ded007fc444c82247488fdd8f244",
            "909b8d32568d490e9360b24431e266ce",
            "a8c826842de945af85aeb31fff1ffa6a",
            "4ab5b254e3584ae891b138a167697b55",
            "27d544ba4c8d4e32ae9334ff07e8220d",
            "a55f2ca1096644d4a04433f53a5dff5c",
            "410ccf71ae134fd7ac43e986d0853e56"
          ]
        },
        "id": "vFkgAjyMR8fa",
        "outputId": "9a10ce96-6d66-4af3-c1b1-d099a7105281"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create Input-Output batches for the dataset"
      ],
      "metadata": {
        "id": "X_qRtn_WSbV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "#block size = context window\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "gak79CZESkjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA1SVp1hkjKy"
      },
      "source": [
        "## Step 4: Define the SLM Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                       .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "9friaxWABOA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-20T15:36:54.88378Z",
          "iopub.status.busy": "2024-06-20T15:36:54.883342Z",
          "iopub.status.idle": "2024-06-20T15:37:07.278493Z",
          "shell.execute_reply": "2024-06-20T15:37:07.277342Z",
          "shell.execute_reply.started": "2024-06-20T15:36:54.883753Z"
        },
        "id": "uRm6WlvfkjKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,     # use the tokenizer's vocab size\n",
        "    block_size=128,       # or whatever context size you're training with\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Define the loss function"
      ],
      "metadata": {
        "id": "C_a8Rd-0S_WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "La2Aun_nTBzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Define SLM Training Configuration Part 1"
      ],
      "metadata": {
        "id": "UvqWPUstTRXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QQyayhnkjK5",
        "outputId": "16387211-88fb-4b07-834e-d33c83ca2734",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 20000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Define SLM Training Configuration Part 2"
      ],
      "metadata": {
        "id": "cmWj6YcKTW_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMkO3FlNkjK6",
        "outputId": "2c801c6f-6abf-4763-adb6-357d6851406c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Pre-train the SLM"
      ],
      "metadata": {
        "id": "Nz8fPSKNTY3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2033566e8cce4494935246c168fbc01d",
            "b52bbe755de341999e29551798abdc88",
            "fdcae12ebba34e628a74599d69843634",
            "ac0eac62e39442739d20eb4e621a1fd0",
            "05d05590394e4373954d29a7d664249f",
            "c5e0b999ff3b440f848eda305afce335",
            "d5d7746619334ad08bbc8b6b7e72b8af",
            "a9c4f92f2eeb474f86e709bb70532d37",
            "f36ea01d74ee41f5aac6f077373d30ef",
            "7aa7d2f58f7841eaafda7dae522ea894",
            "7977d49b41d8486991ae933eee595f95"
          ]
        },
        "id": "t0l-YhockjK6",
        "outputId": "73540e04-fc1b-4db2-ed5a-1922fcbcf2ec",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Plot the SLM Loss Function"
      ],
      "metadata": {
        "id": "pdzhSo_7TcgI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "KSWpvAnakjK6",
        "outputId": "5e6a0588-267b-463d-ad58-9f30d3a82e7f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ocyyQwkjK6"
      },
      "source": [
        "## Step 10: Run SLM Inference on our trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:45:44.322237Z",
          "iopub.status.busy": "2024-06-20T15:45:44.321316Z",
          "iopub.status.idle": "2024-06-20T15:45:46.887084Z",
          "shell.execute_reply": "2024-06-20T15:45:46.886126Z",
          "shell.execute_reply.started": "2024-06-20T15:45:44.322203Z"
        },
        "id": "06NrdWKdkjK7",
        "outputId": "9aa3a5e7-345e-4f22-ea88-601f8090b3db",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Load the model\n",
        "model = GPT(config)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:44:36.64937Z",
          "iopub.status.busy": "2024-06-20T15:44:36.64896Z",
          "iopub.status.idle": "2024-06-20T15:45:14.425576Z",
          "shell.execute_reply": "2024-06-20T15:45:14.424712Z",
          "shell.execute_reply.started": "2024-06-20T15:44:36.649341Z"
        },
        "id": "K8PgWXb-kjK7",
        "outputId": "edc1b0c1-ffa9-4dff-d08e-f0f71856b9f0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentence = \"Once upon a time there was a pumpkin.\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A little girl went to the woods\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZsL_mONRZAO",
        "outputId": "8b8c69ab-1cad-421c-adc6-5decbe4363e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_2WjvUszhIe"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}